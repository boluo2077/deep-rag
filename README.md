<a id="top"></a>

<div align="center">

# [ğŸ” Deep RAG: Teaching AI to Truly "Understand" Your Knowledge Base](%F0%9F%94%8D%20Deep%20RAG%3A%20Teaching%20AI%20to%20Truly%20%22Understand%22%20Your%20Knowledge%20Base.md)

[![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Node.js 16+](https://img.shields.io/badge/node-16+-green.svg)](https://nodejs.org/)

**[ English | [ä¸­æ–‡](./README.zh-CN.md) ]**

[Features](#-features) â€¢ [Quick Start](#-quick-start) â€¢ [How It Works](#-how-it-works) â€¢ [Architecture](#-architecture) â€¢ [Configuration](#-configuration)

---

![Negation Exclusion.png](Knowledge-Base-File-Summary/Negation%20Exclusion.png)

<details>
<summary>ğŸ“¸ Click to see more example images</summary>

<br>

![Finding Extremes.png](Knowledge-Base-File-Summary/Finding%20Extremes.png)
![Comparative Synthesis.png](Knowledge-Base-File-Summary/Comparative%20Synthesis.png)
![Multi-hop Reasoning.png](Knowledge-Base-File-Summary/Multi-hop%20Reasoning.png)
![Global Understanding.png](Knowledge-Base-File-Summary/Global%20Understanding.png)

</details>

</div>

---

## ğŸŒŸ Why Deep RAG?

### The Problem with Traditional RAG

Traditional RAG systems:
- âŒ Split documents into fragments, losing structure
- âŒ Can only "find similar" content, not "find opposite" 
- âŒ Struggle with numerical comparisons and negation logic
- âŒ Cannot perform multi-hop reasoning across documents
- âŒ Lack global understanding for aggregation queries

### The Deep RAG Solution

```
Traditional RAG: Fragment documents â†’ Retrieve chunks â†’ Feed to model
Deep RAG:        Preserve structure â†’ Give model a "map" â†’ Let model navigate
```

Deep RAG provides:
- âœ… **File Summary as Knowledge Map**: LLM sees the entire structure
- âœ… **Active Navigation**: Model retrieves what it needs, when it needs it
- âœ… **Multi-round Retrieval**: Supports complex multi-hop reasoning
- âœ… **Complete Context**: Retrieves full files/directories, not fragments

> **Want to learn more about `Deep RAG`? Welcome to read the intuitive and easy-to-understand article: [ğŸ” Deep RAG: Teaching AI to Truly "Understand" Your Knowledge Base](%F0%9F%94%8D%20Deep%20RAG%3A%20Teaching%20AI%20to%20Truly%20%22Understand%22%20Your%20Knowledge%20Base.md)**

---

## ğŸ¯ Features

### Core Capabilities

| Capability | Traditional RAG | Deep RAG |
|-----------|----------------|----------|
| **Negation Queries** ("except", "besides") | âŒ | âœ… |
| **Numerical Comparison** ("greater than", "less than") | âŒ | âœ… |
| **Finding Extremes** ("maximum", "minimum") | âŒ | âœ… |
| **Cross-document Comparison** | âŒ | âœ… |
| **Temporal Reasoning** ("last year", "previous") | âŒ | âœ… |
| **Multi-turn Memory** | âŒ | âœ… |
| **Multi-hop Reasoning** | âŒ | âœ… |
| **Global Aggregation** | âŒ | âœ… |

### Technical Features

- ğŸ”Œ **Universal LLM Support**: OpenAI, Anthropic, Google Gemini, or any OpenAI-compatible API
- ğŸ› ï¸ **Dual Tool Calling Modes**: Function Calling + ReAct
- ğŸ¨ **Modern Web UI**: Built with React + TypeScript + Vite
- âš¡ **Streaming Responses**: Real-time response streaming
- ğŸ”§ **Easy Configuration**: Web-based .env editor
- ğŸ“Š **Tool Call Visualization**: See what the AI is doing

---

## ğŸš€ Quick Start

### Prerequisites

- **Python 3.8+**
- **Node.js 16+**
- **An LLM API Key** (OpenAI, Google Gemini, Anthropic, or compatible)

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/boluo2077/deep-rag.git
cd deep-rag
```

2. **Configure environment variables**
```bash
cp .env.example .env
# Edit .env with your API keys
```

3. **Start the application**
```bash
./start.sh
```

**Quick start (skip dependency checks):**
```bash
./start.sh --fast
```

The script will:
- âœ… Create Python virtual environment
- âœ… Install backend dependencies
- âœ… Install frontend dependencies  
- âœ… Start backend server (http://localhost:8000)
- âœ… Start frontend dev server (http://localhost:5173)
- âœ… Open browser automatically

### Manual Start (Alternative)

**Backend:**
```bash
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
uvicorn backend.main:app --host 0.0.0.0 --port 8000
```

**Frontend:**
```bash
cd frontend
npm install
npm run dev
```

### Stop/Restart

```bash
./stop.sh
```

```bash
./restart.sh
```

```bash
./restart.sh --full
```

---

## ğŸ’¡ How It Works

### 1. Knowledge Base Structure

Instead of splitting documents into chunks, Deep RAG preserves your file structure:

```
Knowledge-Base/
â”œâ”€ Product-Line-A-Smartwatch-Series/
â”‚  â”œâ”€ SW-2100-Flagship.md
â”‚  â”œâ”€ SW-1800-Business.md
â”‚  â””â”€ SW-1500-Sport.md
â”œâ”€ 2023-Market-Layout/
â”‚  â”œâ”€ East-China-Region.md
â”‚  â””â”€ South-China-Region.md
â””â”€ Supplier-Partnership-Records/
   â””â”€ Display-Supplier-CrystalVision.md
```

### 2. File Summary Generation

Generate a structured summary of your knowledge base:

```bash
cd Knowledge-Base-File-Summary
python generate.py
```

This creates a "knowledge map" that looks like:
```
Product-Line-A-Smartwatch-Series/
â”œâ”€ SW-2100-Flagship.md: 2.1" AMOLED, 72h battery, IP68, $2999
â”œâ”€ SW-1800-Business.md: 1.8" LCD, 48h battery, IP67, $1899
â””â”€ SW-1500-Sport.md: 1.5" TFT, 36h battery, IP68, $999
```

### 3. System Prompt Integration

The file summary is injected into the system prompt, giving the LLM:
- ğŸ“ Overview of all available knowledge
- ğŸ—ºï¸ File paths for targeted retrieval
- ğŸ¯ Ability to plan multi-step queries

### 4. Active Retrieval

When answering questions, the LLM can:
```python
retrieve_files([
    "Product-Line-A-Smartwatch-Series/SW-2100-Flagship.md",  # Specific file
    "2023-Market-Layout/",                                     # Entire directory
    "/"                                                        # All files
])
```

---

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       Frontend (React)                       â”‚
â”‚  â€¢ Chat Interface  â€¢ Config Panel  â€¢ System Prompt Viewer   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚ HTTP/SSE
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Backend (FastAPI)                         â”‚
â”‚  â€¢ LLM Provider Abstraction  â€¢ Tool Calling Handler         â”‚
â”‚  â€¢ Knowledge Base Manager    â€¢ ReAct Mode Support           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
   â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”          â”Œâ”€â”€â”€â”´â”€â”€â”€â”
   â”‚Knowledgeâ”‚         â”‚  LLM  â”‚          â”‚ Tools â”‚
   â”‚  Base   â”‚         â”‚  API  â”‚          â”‚(Func/ â”‚
   â”‚  Files  â”‚         â”‚       â”‚          â”‚ReAct) â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Project Structure

```
deep-rag/
â”œâ”€â”€ backend/                   # FastAPI backend
â”‚   â”œâ”€â”€ main.py               # API endpoints
â”‚   â”œâ”€â”€ config.py             # Configuration management
â”‚   â”œâ”€â”€ llm_provider.py       # LLM provider abstraction
â”‚   â”œâ”€â”€ knowledge_base.py     # Knowledge base operations
â”‚   â”œâ”€â”€ prompts.py            # System prompts & tools
â”‚   â”œâ”€â”€ react_handler.py      # ReAct mode handler
â”‚   â””â”€â”€ models.py             # Pydantic models
â”œâ”€â”€ frontend/                  # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx           # Main app component
â”‚   â”‚   â”œâ”€â”€ components/       # React components
â”‚   â”‚   â””â”€â”€ api.ts            # API client
â”‚   â””â”€â”€ package.json
â”œâ”€â”€ Knowledge-Base/            # Your documents
â”œâ”€â”€ Knowledge-Base-Chunks/     # Chunked documents (optional)
â”œâ”€â”€ Knowledge-Base-File-Summary/
â”‚   â”œâ”€â”€ generate.py           # Summary generator
â”‚   â””â”€â”€ summary.txt           # Generated summary
â”œâ”€â”€ .env.example              # Environment config template
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ start.sh                  # Start script
â”œâ”€â”€ stop.sh                   # Stop script
â””â”€â”€ restart.sh                # Restart script
```

---

## âš™ï¸ Configuration

### Environment Variables

Edit `.env` to configure:

```bash
# LLM Provider (openai, google, anthropic, custom)
API_PROVIDER=google

# Tool Calling Mode (function, react)
TOOL_CALLING_MODE=function

# Model Parameters
TEMPERATURE=0
MAX_TOKENS=8192

# Knowledge Base Paths
KNOWLEDGE_BASE_PATH=./Knowledge-Base
KNOWLEDGE_BASE_FILE_SUMMARY=./Knowledge-Base-File-Summary/summary.txt

# Google Gemini
GOOGLE_API_KEY=your_google_key
GOOGLE_BASE_URL=https://generativelanguage.googleapis.com/v1beta/openai
GOOGLE_MODEL=gemini-2.5-flash-lite

# OpenAI
OPENAI_API_KEY=your_openai_key
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_MODEL=gpt-4.1-mini

# Anthropic Claude
ANTHROPIC_API_KEY=your_anthropic_key
ANTHROPIC_BASE_URL=https://api.anthropic.com/v1
ANTHROPIC_MODEL=claude-3-5-sonnet-20241022

# Custom Provider (any OpenAI-compatible API)
CUSTOM_API_KEY=your_api_key
CUSTOM_BASE_URL=https://your-api.com/v1/chat/completions
CUSTOM_MODEL=your-model
```

### Adding New LLM Providers

No code changes needed! Just add to `.env`:

```bash
PROVIDER_NAME_API_KEY=your_key
PROVIDER_NAME_BASE_URL=https://api.provider.com/v1
PROVIDER_NAME_MODEL=model-name

API_PROVIDER=provider_name
```

### Tool Calling Modes

**Function Calling Mode** (Recommended)
- For models with native function calling support
- Examples: GPT-4+, Gemini 1.5+, Claude 3.5+
- More reliable and structured

**ReAct Mode**
- For models without function calling
- Uses prompt-based reasoning and action
- Compatible with any text-completion model

---

<div align="center">

**If this project helps you, please click the â­ Star in the top right corner!**

*Your Star is our motivation to keep improving ğŸ’ª*

![Star History Chart](https://api.star-history.com/svg?repos=boluo2077/deep-rag&type=Date)

---

<a href="#top">
  <img src="https://img.shields.io/badge/â¬†ï¸-Back_to_Top-blue?style=for-the-badge" alt="Back to Top">
</a>

</div>